---
title:  "TF Lite 소개"
#iexcerpt: ""
categories:
  - machine_learning
#tags:
#  - Blog
last_modified_at: 2021-06-29
---



## TensorFlow Lite

- 모바일 기기, 임베디드 기기 및 IoT 기기에서 TensorFlow 모델을 실행할 수 있도록 지원하는 도구 모음
- 기기 내 지연 시간이 짧고 바이너리 크기가 작은 머신러닝 추론을 지원





### Edge Machine Learning

엣지 컴퓨팅은 서로 다른 위치에서 가끔씩 클라우드에 연결하거나 클라우드에 연결하지 않고 컴퓨팅 리소스 및 의사 결정 기능을 활용하는 기술을 말함. 엣지 머신러닝은 클라우드가 아닌 각 단말 단에서 활용되는 머신러닝 기술을 표현할 때 사용됨. 엣지 머신러닝에서의 단말은 모바일 및 IoT 등의 작은 컴퓨팅 파워를 가진 디바이스를 의미함

- 지연시간: 서버에 접근이 없이 단말에서 수행되므로 응답속도가 빠르다.
- 개인정보 보호: 데이터가 기기를 벗어나지 않으므로 privacy 노출 위험이 없음
- 연결: 클라우드 서버에 접근이 불필요 하므로 인터넷 연결을 필요로 하지 않음
- 전력 소비: 네트워크 연결을 위한 전력 소모 없이, 작은 사이즈의 모델 구동으로 컴퓨팅 전력 소모가 적음



### TF Lite 주요 특징

TF Lite는 작은 컴퓨팅 파워를 가진 디바이스에서도 원활하게 돌아갈 수 있도록 초점이 맞추어 짐

- 기기 내 ML에 맞게 조정된 해석기(Interpreter) 지원
    - 작은 바이너리 크기로 기기 내 애플리케이션에 최적화된 핵심 연산자 세트를 지원하므로, TensorFlow의 전체 연산자 세트를 지원하지는 않음
- 다양한 플랫폼 지원
    - Android, IOS, 내장형 Linux 기기 및 마이크로 컨트롤러를 포함하며, 주요 하드웨어에 대해 플랫폼 API를 통해 추론 가속화를 지원
- 여러 언어용 API
    - 자바, Switft, Objective-C, C++, Python 등을 지원
- 모델 최적화 도구
    - 양자화(Quantization)을 비롯한 모델 사이즈를 줄이기 위해 다양한 툴을 제공하고, 이러한 툴을 통해 모델 사이즈를 줄이고, 속도를 향상시키면서 정확도의 손실을 최소화 함 
- 효율적인 모델 형식
    - TF Lite는 FlatBuffer를 사용하여 작고 이식성 좋은 모델 포맷을 구현 함. 반면, TensorFlow는 Protocol Bufferf를 사용함



### TF Lite 개발 workflow

TF Lite의 개발 workflow는 TensorFlow와 약간의 차이가 있다. TF Lite에서는 학습을 지원하지 않는다. (TF Lite 로드맵에 의하면 온 디바이스 학습 지원 예정) 그래서 TensorFlow로 모델 코드를 작성하고 학습을 진행하여, 모델을 만들고 난 후 tflite 모델로 변환 한 후 Edge 디바이스에서 추론(Inference)을 수행한다.

1. 모델 선택
    - TF Lite에서 구동할 TensorFlow 모델을 사전에 학습한 모델을 선택 또는 학습된 모델을 재학습시키거나, 직접 코드를 작성하여 학습시킨다.
    - TensorFlow에서는 통상 pb 바일을 사용하는데, 이 pb 파일 형태의 모델을 만드는 단계이다.
2. TF Lite 모델로 변환
    - TensorFlow용으로 작성한 pb 모델 파일을 TF Lite 포맷인 tflite 파일로 변환(convert)한다.
    - 몇 줄의 Python을 사용하여 모델을 변환할 수 있고, 별도의 툴도 지원하므로 상황에 맞게 선택한다.
3. 기기에 배포
    - tflite 모델 파일을 실제 구동할 디바이스에서 추론(Inference) 수행을 배포한다.
    - Edge 단 배포는 Java, Swift, Objective-C, C++, Python과 같이 다양한 언어로 제공되는 API 세트가 있다.
4. 모델 최적화
    - TFLITE 모델 생성 완료 후, 모델 파일 최적화를 수행할 수 있다. 이 과정을 통해 모델 사이즈를 더 작게 만들어서 추론(Inference) 시간을 절약할 수 있다,
    - 모델 사이즈를 줄이는 과정을 정확도에 영향을 미치기 때문에 이를 최소화하면서 모델의 크기를 줄이는 것을 목표로 한다.